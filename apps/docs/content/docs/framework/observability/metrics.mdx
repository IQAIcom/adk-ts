---
title: Metrics
description: Monitor agent performance with counters, histograms, and token usage tracking
---

import { Cards, Card } from "fumadocs-ui/components/card";
import { Callout } from "fumadocs-ui/components/callout";

ADK-TS collects metrics for agent performance, tool usage, LLM interactions, and error rates. All metrics follow OpenTelemetry standards and are exported via OTLP.

## Available Metrics

The telemetry system automatically collects metrics for all agent operations. Metrics are exported at regular intervals (configurable via `metricExportIntervalMs`). Metrics can be monitored using counters and histograms.

### Counters

Counters track the total number of operations:

| Metric                  | Description             | Labels                                             |
| ----------------------- | ----------------------- | -------------------------------------------------- |
| `adk.agent.invocations` | Total agent invocations | `agent.name`, `environment`, `status`              |
| `adk.tool.executions`   | Total tool executions   | `tool.name`, `agent.name`, `environment`, `status` |
| `adk.llm.calls`         | Total LLM calls         | `model`, `agent.name`, `environment`, `status`     |
| `adk.errors`            | Total errors            | `error_type`, `context`                            |

### Histograms

Histograms track distributions of values:

| Metric                  | Description                | Labels                                             | Unit  |
| ----------------------- | -------------------------- | -------------------------------------------------- | ----- |
| `adk.agent.duration`    | Agent execution duration   | `agent.name`, `environment`, `status`              | ms    |
| `adk.tool.duration`     | Tool execution duration    | `tool.name`, `agent.name`, `environment`, `status` | ms    |
| `adk.llm.duration`      | LLM call duration          | `model`, `agent.name`, `environment`, `status`     | ms    |
| `adk.llm.tokens`        | Total tokens per LLM call  | `model`, `agent.name`, `environment`, `status`     | count |
| `adk.llm.tokens.input`  | Input tokens per LLM call  | `model`, `agent.name`, `environment`, `status`     | count |
| `adk.llm.tokens.output` | Output tokens per LLM call | `model`, `agent.name`, `environment`, `status`     | count |

## Metric Labels

All metrics include labels for filtering and aggregation:

- **`agent.name`** - Name of the agent
- **`tool.name`** - Name of the tool (for tool metrics)
- **`model`** - LLM model name (for LLM metrics)
- **`environment`** - Deployment environment
- **`status`** - Operation status (`success`, `error`)
- **`error_type`** - Type of error (for error metrics)
- **`context`** - Error context (for error metrics)

## Metric Dimensions

When recording metrics, you can include additional dimensions:

```typescript
telemetryService.recordAgentInvocation({
  agentName: "my-agent",
  environment: "production",
  status: "success",
  // Additional dimensions can be added via resource attributes
});
```

Resource attributes are automatically included as metric labels, allowing you to filter and aggregate by deployment, team, or any custom attribute.

## Recording Custom Metrics

You can manually record metrics for custom operations:

```typescript
import { telemetryService } from "@iqai/adk";

// Record agent invocation
telemetryService.recordAgentInvocation({
  agentName: "my-agent",
  environment: "production",
  status: "success",
});

// Record agent duration
telemetryService.recordAgentDuration(1500, {
  agentName: "my-agent",
  environment: "production",
  status: "success",
});

// Record tool execution
telemetryService.recordToolExecution({
  toolName: "search_web",
  agentName: "my-agent",
  environment: "production",
  status: "success",
});

// Record LLM tokens
telemetryService.recordLlmTokens(150, 75, {
  model: "gpt-4",
  agentName: "my-agent",
  environment: "production",
  status: "success",
});
```

## Metric Export

Metrics are automatically exported to your configured OTLP endpoint at regular intervals. The default export interval is 60 seconds (60000ms), but you can configure it:

```typescript
await telemetryService.initialize({
  appName: "my-app",
  otlpEndpoint: "http://localhost:4318/v1/traces",
  metricExportIntervalMs: 30000, // Export every 30 seconds
});
```

<Callout type="info" title="Export Interval">
  Adjust the export interval based on your needs. More frequent exports provide
  real-time metrics but may increase overhead.
</Callout>

## Viewing Metrics

ADK-TS sends metrics via OTLP to any compatible observability backend. You'll need to set up an external metrics visualization tool separately. Popular options include Grafana (with Tempo or Prometheus), Datadog, New Relic, and Honeycomb.

<Callout type="info" title="No Built-in Viewer">
  ADK-TS does not include a built-in metrics viewer. You need to set up an
  external observability platform to view metrics. See the [Platform
  Integrations](/docs/framework/observability/integrations) guide for setup
  instructions.
</Callout>

### Grafana

Grafana is a popular open-source visualization tool that can display metrics from various backends:

1. **Set up Grafana**: See the [Platform Integrations](/docs/framework/observability/integrations#grafana--tempo) guide for Docker setup instructions
2. **Configure OTLP receiver** in your observability backend
3. **Create dashboards** with queries for ADK-TS metrics
4. **Set up alerts** based on metric thresholds

Example Grafana queries:

- `rate(adk_agent_invocations_total[5m])` - Agent invocation rate
- `histogram_quantile(0.95, adk_llm_duration_bucket)` - 95th percentile LLM latency
- `sum(rate(adk_errors_total[5m]))` - Error rate

### Prometheus

Prometheus is a metrics database that can be used with Grafana for visualization. If using Prometheus, configure the OpenTelemetry Collector to export metrics in Prometheus format:

```yaml
# otel-collector-config.yaml
exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    metrics:
      receivers: [otlp]
      exporters: [prometheus]
```

## Common Use Cases

| Use Case        | Metrics to Monitor                                              |
| --------------- | --------------------------------------------------------------- |
| **Performance** | `adk.agent.duration`, `adk.tool.duration`, `adk.llm.duration`   |
| **Costs**       | `adk.llm.tokens.input`, `adk.llm.tokens.output`                 |
| **Reliability** | `adk.errors`, error rates by `error_type`                       |
| **Usage**       | `adk.agent.invocations`, `adk.tool.executions`, `adk.llm.calls` |

## Related Topics

<Cards>
  <Card
    title="ðŸ” Distributed Tracing"
    description="Correlate metrics with traces for debugging"
    href="/docs/framework/observability/tracing"
  />
  <Card
    title="ðŸ”Œ Platform Integrations"
    description="Visualize metrics in Grafana, Datadog, and other platforms"
    href="/docs/framework/observability/integrations"
  />
  <Card
    title="ðŸ”’ Production Deployment"
    description="Configure optimal metric collection for production"
    href="/docs/framework/observability/production"
  />
</Cards>
