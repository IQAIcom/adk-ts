---
title: Callbacks
description: Observe, customize, and control agent behavior with powerful callback mechanisms
---

import { Cards, Card } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';


Callbacks are a cornerstone feature of ADK, providing a powerful mechanism to hook into an agent's execution process. They allow you to observe, customize, and even control the agent's behavior at specific, predefined points without modifying the core ADK framework code.

**What are they?** In essence, callbacks are standard functions that you define. You then associate these functions with an agent when you create it. The ADK framework automatically calls your functions at key stages, letting you observe or intervene. Think of it like checkpoints during the agent's process:

-   **Before the agent starts its main work on a request, and after it finishes:** When you ask an agent to do something (e.g., answer a question), it runs its internal logic to figure out the response.
-   The `Before Agent` callback executes _right before_ this main work begins for that specific request.
-   The `After Agent` callback executes _right after_ the agent has finished all its steps for that request and has prepared the final result, but just before the result is returned.
-   This "main work" encompasses the agent's _entire_ process for handling that single request. This might involve deciding to call an LLM, actually calling the LLM, deciding to use a tool, using the tool, processing the results, and finally putting together the answer. These callbacks essentially wrap the whole sequence from receiving the input to producing the final output for that one interaction.
-   **Before sending a request to, or after receiving a response from, the Large Language Model (LLM):** These callbacks (`Before Model`, `After Model`) allow you to inspect or modify the data going to and coming from the LLM specifically.
-   **Before executing a tool (like a Python function or another agent) or after it finishes:** Similarly, `Before Tool` and `After Tool` callbacks give you control points specifically around the execution of tools invoked by the agent.

<Mermaid
  chart="
flowchart TB
  %% Top lane: Tools
  subgraph T[Tools]
    direction LR
    BT[before_tool_callback] --> TL[Tools] --> AT[after_tool_callback]
  end

  %% Middle lane: Agent
  A[before_agent_callback] --> AG[Agent] --> AA[after_agent_callback]

  %% Bottom lane: Model
  subgraph M[Model]
    direction LR
    BM[before_model_callback] --> ML[Model] --> AM[after_model_callback]
  end

  %% Cross-links from Agent to Tools/Model
  AG -.-> TL
  AG -.-> ML
"/>

## How Callbacks Work

### Callback Registration

Callbacks are registered during agent creation:

```typescript
import { LlmAgent, CallbackContext, LlmRequest, LlmResponse } from '@iqai/adk';

// Define callback functions
const beforeModelCallback = ({ callbackContext, llmRequest }: {
  callbackContext: CallbackContext;
  llmRequest: LlmRequest;
}): LlmResponse | undefined => {
  console.log(`Processing request for agent: ${callbackContext.agentName}`);

  // Return undefined to proceed normally
  // Return LlmResponse to skip LLM call
  return undefined;
};

const afterAgentCallback = (callbackContext: CallbackContext) => {
  console.log(`Agent ${callbackContext.agentName} completed processing`);
  return undefined; // Allow normal completion
};

// Create agent with callbacks
const agent = new LlmAgent({
  name: "callback_agent",
  model: "gemini-2.5-flash",
  description: "Agent with callbacks",
  instruction: "You are helpful",
  beforeModelCallback,
  afterAgentCallback
});
```

### Control Flow

Callbacks control execution through their return values:

**Return `undefined`**: Allow normal execution to continue
**Return specific object**: Override or skip the default behavior

- `beforeAgentCallback` â†’ `Content`: Skip agent execution, use returned content
- `beforeModelCallback` â†’ `LlmResponse`: Skip LLM call, use returned response
- `afterAgentCallback` â†’ `Content`: Replace agent's output
- `afterModelCallback` â†’ `LlmResponse`: Replace LLM response

## Callback Types

The framework provides different callback types for various execution stages:

<Cards>
  <Card
    title="ðŸš€ Agent Lifecycle"
    description="Before and after complete agent processing with CallbackContext"
    href="/docs/framework/callbacks/types#agent-lifecycle-callbacks"
  />

  <Card
    title="ðŸ§  Model Interactions"
    description="Before and after LLM communications with request/response access"
    href="/docs/framework/callbacks/types#llm-interaction-callbacks"
  />

  <Card
    title="ðŸ”§ Tool Execution"
    description="Before and after tool invocations with ToolContext"
    href="/docs/framework/callbacks/types#tool-execution-callbacks"
  />
</Cards>

## Documentation Structure

<Cards>
  <Card
    title="ðŸ“‹ Callback Types"
    description="Detailed reference for all callback types and their contexts"
    href="/docs/framework/callbacks/types"
  />

  <Card
    title="ðŸŽ¨ Design Patterns"
    description="Common patterns for guardrails, caching, logging, and more"
    href="/docs/framework/callbacks/design-patterns"
  />

  <Card
    title="ðŸ”§ Context Patterns"
    description="Working with CallbackContext and ToolContext effectively"
    href="/docs/framework/callbacks/context-patterns"
  />
</Cards>

## Quick Example

Here's a simple callback that adds logging and implements a basic guardrail:

```typescript
import { LlmAgent, CallbackContext, LlmRequest, LlmResponse, Content } from '@iqai/adk';

// Before model callback with guardrail
const guardrailCallback = ({ callbackContext, llmRequest }: {
  callbackContext: CallbackContext;
  llmRequest: LlmRequest;
}): LlmResponse | undefined => {
  // Log the request
  console.log(`[${callbackContext.invocationId}] Processing LLM request`);

  // Check for blocked content in the last user message
  const lastContent = llmRequest.contents?.[llmRequest.contents.length - 1];
  const lastMessage = lastContent?.parts?.[0]?.text || "";

  if (lastMessage.toLowerCase().includes("blocked")) {
    // Return response to skip LLM call
    return new LlmResponse({
      content: {
        role: "model",
        parts: [{ text: "I cannot process requests containing blocked content." }]
      }
    });
  }

  return undefined; // Proceed with normal LLM call
};

// After agent callback for state management
const stateManagementCallback = (callbackContext: CallbackContext): Content | undefined => {
  // Update interaction count in state
  const currentCount = callbackContext.state.get('interaction_count') || 0;
  callbackContext.state.set('interaction_count', currentCount + 1);

  console.log(`Interaction count: ${currentCount + 1}`);
  return undefined; // Use agent's original output
};

const agent = new LlmAgent({
  name: "guarded_agent",
  model: "gemini-2.5-flash",
  description: "Agent with guardrails and state tracking",
  instruction: "You are a helpful assistant",
  beforeModelCallback: guardrailCallback,
  afterAgentCallback: stateManagementCallback
});
```

## Context Objects

Callbacks receive different context objects depending on their type:

### CallbackContext

Used in agent lifecycle and LLM interaction callbacks:

- **State Management**: Read/write session state with automatic delta tracking
- **Artifact Operations**: Save and load files with `saveArtifact()` and `loadArtifact()`
- **Invocation Metadata**: Access to invocation ID, agent name, user content
- **Event Actions**: Control event generation and side effects

### ToolContext

Used in tool execution callbacks, extends CallbackContext with:

- **Function Call ID**: Identifier for the specific tool invocation
- **Authentication**: Credential management with `requestCredential()`
- **Artifact Listing**: Enumerate session artifacts with `listArtifacts()`
- **Memory Search**: Query memory service with `searchMemory()`
- **Summarization Control**: Skip LLM summarization of tool results

## Common Use Cases

### Security & Compliance

- **Input Validation**: Check user inputs for safety and compliance
- **Output Filtering**: Remove sensitive information from responses
- **Access Control**: Verify user permissions before tool execution
- **Audit Logging**: Track all interactions for compliance

### Performance Optimization

- **Response Caching**: Cache LLM responses to avoid redundant calls
- **Request Batching**: Combine multiple requests for efficiency
- **Resource Management**: Monitor and control resource usage
- **Early Termination**: Stop processing when conditions are met

### User Experience

- **Dynamic Instructions**: Adapt agent behavior based on user context
- **Progress Tracking**: Update users on long-running operations
- **Error Recovery**: Provide graceful fallbacks for failures
- **Personalization**: Customize responses based on user preferences

## Best Practices

### Design Principles

- **Single Responsibility**: Each callback should have one clear purpose
- **Performance Awareness**: Avoid blocking operations in callbacks
- **Error Handling**: Use try-catch blocks and graceful degradation
- **State Management**: Be deliberate about state changes and their scope

### Implementation Guidelines

- **Type Safety**: Use proper TypeScript types for all callback parameters
- **Testing**: Unit test callbacks with mock context objects
- **Documentation**: Document callback behavior and side effects clearly
- **Idempotency**: Design callbacks to be safe for retry scenarios

## Related Topics

<Cards>
  <Card
    title="ðŸ¤– Agents"
    description="Learn how agents integrate callbacks into their execution"
    href="/docs/framework/agents"
  />

  <Card
    title="ðŸ“Š Runtime"
    description="Understanding callback integration with runtime execution"
    href="/docs/framework/runtime"
  />

  <Card
    title="ðŸ’¬ Sessions"
    description="Session state management through callbacks"
    href="/docs/framework/sessions"
  />

  <Card
    title="ðŸ”§ Tools"
    description="Tool execution control through callbacks"
    href="/docs/framework/tools"
  />
</Cards>